 @article{mach-learning-matters, title={Machine Learning that Matters}, url={http://arxiv.org/abs/1206.4656}, abstractNote={Much of current machine learning (ML) research has lost its connection to problems of import to the larger world of science and society. From this perspective, there exist glaring limitations in the data sets we investigate, the metrics we employ for evaluation, and the degree to which results are communicated back to their originating domains. What changes are needed to how we conduct research to increase the impact that ML has? We present six Impact Challenges to explicitly focus the field?s energy and attention, and we discuss existing obstacles that must be addressed. We aim to inspire ongoing discussion and focus on ML that matters.}, note={arXiv: 1206.4656}, journal={arXiv:1206.4656 [cs, stat]}, author={Wagstaff, Kiri}, year={2012}, month={Jun} }

 @article{water-pipe-risk, title={Water Pipe Condition Assessment: A Hierarchical Beta Process Approach for Sparse Incident Data}, volume={95}, ISSN={0885-6125}, DOI={10.1007/s10994-013-5386-z}, abstractNote={Prediction of water pipe condition through statistical modelling is an important element for the risk management strategy of water distribution systems. In this work a hierarchical nonparametric model has been used to enhance the performance of pipe condition assessment. The main aims of this work are three-fold: (1) For sparse incident data, develop an efficient approximate inference algorithm based on hierarchical beta process. (2) Apply the hierarchical beta process based method to water pipe condition assessment. (3) Interpret the outcomes in financial terms usable by the water utilities. The experimental results show superior performance of the proposed method compared to current best practice methods, leading to substantial savings on reactive repairs and maintenance, as well as improved prioritization for capital expenditure.}, number={1}, journal={Machine Learning}, author={Li, Zhidong and Zhang, Bang and Wang, Yang and Chen, Fang and Taib, Ronnie and Whiffin, Vicky and Wang, Yi}, year={2014}, month={Apr}, pages={11–26} }

 @article{aviation-turbulence, title={Using random forests to diagnose aviation turbulence.}, volume={95}, ISSN={0885-6125}, DOI={10.1007/s10994-013-5346-7}, abstractNote={Abstract: Atmospheric turbulence poses a significant hazard to aviation, with severe encounters costing airlines millions of dollars per year in...}, number={1}, journal={Machine Learning}, author={Williams, J. K.}, year={2014}, pages={51–70} }

 @article{drug-discovery, title={Massively Multitask Networks for Drug Discovery}, url={http://arxiv.org/abs/1502.02072}, abstractNote={Massively multitask neural architectures provide a learning framework for drug discovery that synthesizes information from many distinct biological sources. To train these architectures at scale, we gather large amounts of data from public sources to create a dataset of nearly 40 million measurements across more than 200 biological targets. We investigate several aspects of the multitask framework by performing a series of empirical studies and obtain some interesting results: (1) massively multitask networks obtain predictive accuracies significantly better than single-task methods, (2) the predictive power of multitask networks improves as additional tasks and data are added, (3) the total amount of data and the total number of tasks both contribute significantly to multitask improvement, and (4) multitask networks afford limited transferability to tasks not in the training set. Our results underscore the need for greater data sharing and further algorithmic innovation to accelerate the drug discovery process.}, note={arXiv: 1502.02072}, journal={arXiv:1502.02072 [cs, stat]}, author={Ramsundar, Bharath and Kearnes, Steven and Riley, Patrick and Webster, Dale and Konerding, David and Pande, Vijay}, year={2015}, month={Feb} }
 
 @misc{tensorflow-main-paper,
title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={https://www.tensorflow.org/about/bib},
note={(Last accessed on 2019-01-07) Software available from tensorflow.org},
author={
    Mart\'{\i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dandelion~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}
 
@misc{halloc-paper, title={Halloc: A High-Throughput Dynamic Memory Allocator for GPGPU Architectures}, url={http://on-demand.gputechconf.com/gtc/2014/presentations/S4271-halloc-high-throughput-dynamic-memory-allocator.pdf}, author={Adinetz, Andrew}, year={2014}, note={(Last accessed on 2019-01-07)}}

@article{Vinkler2015,
author = {Vinkler, M. and Havran, V.},
title = {Register Efficient Dynamic Memory Allocator for GPUs},
journal = {Computer Graphics Forum},
volume = {34},
number = {8},
pages = {143-154},
year = {2015},
keywords = {dynamic memory allocation, many-core architecture, GPU, CUDA, D.1.3 Programming Techniques: Concurrent Programming Parallel programming D.3.3 Programming Languages: Language Constructs and Features Dynamic storage management D.4.2 Operating Systems: Storage Management Allocation/deallocation strategies},
doi = {10.1111/cgf.12666},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.12666},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.12666},
abstract = {Abstract We compare five existing dynamic memory allocators optimized for GPUs and show their strengths and weaknesses. In the measurements, we use three generic evaluation tests proposed in the past and we add one with a real workload, where dynamic memory allocation is used in building the k-d tree data structure. Following the performance analysis we propose a new dynamic memory allocator and its variants that address the limitations of the existing dynamic memory allocators. The new dynamic memory allocator uses few resources and is targeted towards large and variably sized memory allocations on massively parallel hardware architectures.}
}

 @article{security-system, title={A Smart Security System with Face Recognition}, url={http://arxiv.org/abs/1812.09127}, abstractNote={Web-based technology has improved drastically in the past decade. As a result, security technology has become a major help to protect our daily life. In this paper, we propose a robust security based on face recognition system (SoF). In particular, we develop this system to giving access into a home for authenticated users. The classifier is trained by using a new adaptive learning method. The training data are initially collected from social networks. The accuracy of the classifier is incrementally improved as the user starts using the system. A novel method has been introduced to improve the classifier model by human interaction and social media. By using a deep learning framework - TensorFlow, it will be easy to reuse the framework to adopt with many devices and applications.}, note={arXiv: 1812.09127}, journal={arXiv:1812.09127 [cs, eess]}, author={Nguyen, Trung and Lakshmanan, Barth and Sheng, Weihua}, year={2018}, month={Dec} }

 @article{brain-tumor-detection, title={Automatic Brain Tumor Detection and Segmentation Using U-Net Based Fully Convolutional Networks}, url={http://arxiv.org/abs/1705.03820}, abstractNote={A major challenge in brain tumor treatment planning and quantitative evaluation is determination of the tumor extent. The noninvasive magnetic resonance imaging (MRI) technique has emerged as a front-line diagnostic tool for brain tumors without ionizing radiation. Manual segmentation of brain tumor extent from 3D MRI volumes is a very time-consuming task and the performance is highly relied on operator’s experience. In this context, a reliable fully automatic segmentation method for the brain tumor segmentation is necessary for an efficient measurement of the tumor extent. In this study, we propose a fully automatic method for brain tumor segmentation, which is developed using U-Net based deep convolutional networks. Our method was evaluated on Multimodal Brain Tumor Image Segmentation (BRATS 2015) datasets, which contain 220 high-grade brain tumor and 54 low-grade tumor cases. Crossvalidation has shown that our method can obtain promising segmentation efficiently.}, note={arXiv: 1705.03820}, journal={arXiv:1705.03820 [cs]}, author={Dong, Hao and Yang, Guang and Liu, Fangde and Mo, Yuanhan and Guo, Yike}, year={2017}, month={May} }


@book{bock2001,
  title={Getting It Right: R\&D Methods for Science and Engineering},
  author={Bock, P. and Scheibe, B.},
  isbn={9780121088521},
  lccn={2001089411},
  url={https://books.google.nl/books?id=YS-SaIM3SKAC},
  year={2001},
  publisher={Elsevier Science}
}

@inproceedings{abadi2016,
title	= {TensorFlow: A system for large-scale machine learning},
author	= {Martin Abadi and Paul Barham and Jianmin Chen and Zhifeng Chen and Andy Davis and Jeffrey Dean and Matthieu Devin and Sanjay Ghemawat and Geoffrey Irving and Michael Isard and Manjunath Kudlur and Josh Levenberg and Rajat Monga and Sherry Moore and Derek G. Murray and Benoit Steiner and Paul Tucker and Vijay Vasudevan and Pete Warden and Martin Wicke and Yuan Yu and Xiaoqiang Zheng},
year	= {2016},
URL	= {https://www.usenix.org/system/files/conference/osdi16/osdi16-abadi.pdf},
booktitle	= {12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16)},
pages	= {265--283}
}

===
The below has this good quote:
"These operations will incur significant overheads since serial processing, memory allocation and deallocation greatly limits the performance on the low-frequency processors (i.e.mic cores)."
===
@article{YOU2015,
title = "Scaling Support Vector Machines on modern HPC platforms",
journal = "Journal of Parallel and Distributed Computing",
volume = "76",
pages = "16 - 31",
year = "2015",
note = "Special Issue on Architecture and Algorithms for Irregular Applications",
issn = "0743-7315",
doi = "https://doi.org/10.1016/j.jpdc.2014.09.005",
url = "http://www.sciencedirect.com/science/article/pii/S0743731514001683",
author = "Yang You and Haohuan Fu and Shuaiwen Leon Song and Amanda Randles and Darren Kerbyson and Andres Marquez and Guangwen Yang and Adolfy Hoisie",
keywords = "Machine learning models, Dynamic modeling, Support Vector Machine, Multi- & many-core architectures, Optimization techniques, Performance analysis",
abstract = "Support Vector Machines (SVM) have been widely used in data-mining and Big Data applications as modern commercial databases start to attach an increasing importance to the analytic capabilities. In recent years, SVM was adapted to the field of High Performance Computing for power/performance prediction, auto-tuning, and runtime scheduling. However, even at the risk of losing prediction accuracy due to insufficient runtime information, researchers can only afford to apply offline model training to avoid significant runtime training overhead. Advanced multi- and many-core architectures offer massive parallelism with complex memory hierarchies which can make runtime training possible, but form a barrier to efficient parallel SVM design. To address the challenges above, we designed and implemented MIC-SVM, a highly efficient parallel SVM for x86 based multi-core and many-core architectures, such as the Intel Ivy Bridge CPUs and Intel Xeon Phi co-processor (MIC). We propose various novel analysis methods and optimization techniques to fully utilize the multilevel parallelism provided by these architectures and serve as general optimization methods for other machine learning tools. MIC-SVM achieves 4.4–84× and 18–47× speedups against the popular LIBSVM, on MIC and Ivy Bridge CPUs respectively, for several real-world data-mining datasets. Even compared with GPUSVM, running on the NVIDIA k20x GPU, the performance of our MIC-SVM is competitive. We also conduct a cross-platform performance comparison analysis, focusing on Ivy Bridge CPUs, MIC and GPUs, and provide insights on how to select the most suitable advanced architectures for specific algorithms and input data patterns."
}

=====
We will use it in the main report :D
=====
@incollection{GINSBURG2017345,
title = "Chapter 16 - Application case study—machine learning",
editor = "David B. Kirk and Wen-mei W. Hwu",
booktitle = "Programming Massively Parallel Processors",
publisher = "Morgan Kaufmann",
edition = "Third Edition",
pages = "345 - 367",
year = "2017",
isbn = "978-0-12-811986-0",
doi = "https://doi.org/10.1016/B978-0-12-811986-0.00016-9",
url = "http://www.sciencedirect.com/science/article/pii/B9780128119860000169",
author = "Boris Ginsburg",
keywords = "Convolutional neural network, machine learning, deep learning, matrix–matrix multiplication, forward propagation, gradient backpropagation, training, cuDNN",
abstract = "This chapter provides an application study of how CUDA and GPU computing helped to enable deep learning and revolutionize the field of machine learning. It starts by introducing the basic concepts of convolutional neural networks (CNN). It then shows the CNN code examples that have been accelerated with CUDA. The chapter concludes with an explanation of how the cuDNN library uses a matrix multiplication formulation of the convolution layer computation to improve the speed and utilization of the GPU."
}



??? needed ???
@book{brent_s._baxter_standard_1982,
	address = {New York  {N.Y.}},
	title = {A standard format for digital image exchange},
	isbn = {9780883184080},
	publisher = {Published for the American Association of Physicists in Medicine by the American Institute of Physics},
	author = {{Brent S. Baxter} and {Lewis E. Hitchner} and {Gerald Q. Maguire Jr.}},
	year = {1982}
}

 @inproceedings{Wald_Havran_2006, title={On building fast kd-Trees for Ray Tracing, and on doing that in O(N log N)}, ISBN={978-1-4244-0693-7}, url={doi.ieeecomputersociety.org/10.1109/RT.2006.280216}, DOI={10.1109/RT.2006.280216}, abstractNote={Though a large variety of efficiency structures for ray tracing exist, kd-trees today seem to slowly become the method of choice. In particular, kd-trees built with cost estimation functions such as a surface area heuristic (SAH) seem to be important for reaching high performance. Unfortunately, most algorithms for building such trees have a time complexity of O(N log2 N), or even O(N2). In this paper, we analyze the state of the art in building good kd-trees for ray tracing, and eventually propose an algorithm that builds SAH kd-trees in O(N log N), the theoretical lower bound}, booktitle={IEEE Symposium on Interactive Ray Tracing 2006(RT)}, author={Wald, I. and Havran, V.}, year={2006}, pages={61–69} }
 
 @article{Rademacher_1997, title={Ray Tracing: Graphics for the Masses}, volume={3}, ISSN={1528-4972}, DOI={10.1145/270955.270962}, number={4}, journal={XRDS}, author={Rademacher, Paul}, year={1997}, month={May}, pages={3–7} }

 @inproceedings{Puaut_2002, title={Real-time performance of dynamic memory allocation algorithms}, DOI={10.1109/EMRTS.2002.1019184}, abstractNote={Dynamic memory management is an important aspect of modern software engineering techniques. However developers of real-time systems avoid using it because they fear that the worst-case execution time of the dynamic memory allocation routines is not bounded or is bounded with an excessively large bound. The degree to which this concern is valid is quantified in this paper by giving detailed average and worst-case measurements of the timing performance of a comprehensive panel of dynamic memory allocators. For each allocator we compare its worst-case behavior obtained analytically with the worst timing behavior observed by executing real and synthetic workloads, and with its average timing performance. The results provide a guideline to developers of real-time systems to choose whether to use dynamic memory management or not, and which dynamic allocation algorithm should be preferred from the viewpoint of predictability.}, booktitle={Proceedings 14th Euromicro Conference on Real-Time Systems. Euromicro RTS 2002}, author={Puaut, I.}, year={2002}, month={Jun}, pages={41–49} }

 @article{Ramsundar_Kearnes_Riley_Webster_Konerding_Pande_2015, title={Massively Multitask Networks for Drug Discovery}, url={http://arxiv.org/abs/1502.02072}, abstractNote={Massively multitask neural architectures provide a learning framework for drug discovery that synthesizes information from many distinct biological sources. To train these architectures at scale, we gather large amounts of data from public sources to create a dataset of nearly 40 million measurements across more than 200 biological targets. We investigate several aspects of the multitask framework by performing a series of empirical studies and obtain some interesting results: (1) massively multitask networks obtain predictive accuracies significantly better than single-task methods, (2) the predictive power of multitask networks improves as additional tasks and data are added, (3) the total amount of data and the total number of tasks both contribute significantly to multitask improvement, and (4) multitask networks afford limited transferability to tasks not in the training set. Our results underscore the need for greater data sharing and further algorithmic innovation to accelerate the drug discovery process.}, note={arXiv: 1502.02072}, journal={arXiv:1502.02072 [cs, stat]}, author={Ramsundar, Bharath and Kearnes, Steven and Riley, Patrick and Webster, Dale and Konerding, David and Pande, Vijay}, year={2015}, month={Feb} }

 @misc{fdgmalloc, title={FDGMalloc}, url={https://www.gcc.tu-darmstadt.de/home/proj/fdgmalloc/index.en.jsp}, journal={Graphics, Capture and Massively Parallel Computing – Technische Universität Darmstadt}, author={Graphics, Capture and Massively Parallel Computing}, note={(visited on 2019-01-10)} }

@misc{xmalloc,
name = {XMalloc: A Scalable Lock-free Dynamic Memory Allocator for Many-core Machines - Semantic Scholar}, url={https://www.semanticscholar.org/paper/XMalloc%3A-A-Scalable-Lock-free-Dynamic-Memory-for-Huang-Rodrigues/7d69b0ba423c84e34ef707e7470f3a0c8c2a9d28} }

@INPROCEEDINGS{scatter-alloc,
author={M. Steinberger and M. Kenzel and B. Kainz and D. Schmalstieg},
booktitle={2012 Innovative Parallel Computing (InPar)},
title={ScatterAlloc: Massively parallel dynamic memory allocation for the GPU},
year={2012},
volume={},
number={},
pages={1-10},
keywords={graphics processing units;parallel architectures;resource allocation;storage management;ScatterAlloc;massively parallel dynamic memory allocation;GPU;parallel architectures;graphics processing units;memory request scattering;hashing;allocation speed;data access time;fragmentation;NVIDIA CUDA toolkit;Resource management;Instruction sets;Graphics processing unit;Dynamic scheduling;Memory management;Kernel;dynamic memory allocation;GPU;massively parallel;hashing},
doi={10.1109/InPar.2012.6339604},
ISSN={},
month={May},}


@online{cucumber,
author = {{Google Cloud Blog}},
title = {{"How} a Japanese cucumber farmer is using deep learning and TensorFlow"},
url = {https://cloud.google.com/blog/products/gcp/how-a-japanese-cucumber-farmer-is-using-deep-learning-and-tensorflow/},
note = {Accessed on 2018-09-19}
}

@article{DBLP:HeZRS15,
  author    = {Kaiming He and
               Xiangyu Zhang and
               Shaoqing Ren and
               Jian Sun},
  title     = {Deep Residual Learning for Image Recognition},
  journal   = {CoRR},
  volume    = {abs/1512.03385},
  year      = {2015},
  url       = {http://arxiv.org/abs/1512.03385},
  archivePrefix = {arXiv},
  eprint    = {1512.03385},
  timestamp = {Mon, 13 Aug 2018 16:46:56 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/HeZRS15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@incollection{TRAISTER199099,
title = "Chapter 7 - Pointers and Memory Allocation Functions",
editor = "Robert J. Traister",
booktitle = "Mastering C Pointers",
publisher = "Academic Press",
pages = "99 - 121",
year = "1990",
isbn = "978-0-12-697408-9",
doi = "https://doi.org/10.1016/B978-0-12-697408-9.50011-7",
url = "http://www.sciencedirect.com/science/article/pii/B9780126974089500117",
author = "Robert J. Traister",
abstract = "Publisher Summary
This chapter discusses memory allocation functions in relation to pointers. When a pointer is assigned the memory address of an allocated block of memory, then the pointer has been initialized and is sized to the number of bytes in the storage area. It is then safe to copy objects to this memory block using the pointer for access. The pointer is treated as an array of the same size as the memory block the pointer addresses. Both of the standard C language memory allocation functions will return a NULL character if the amount of memory requested cannot be allocated. Any program that uses either of these functions must test their returns for the NULL. In the Turbo C compiler and most other modern-day equivalents, malloc()and calloc() return pointers of type void. This means that the return address value can assume any legal pointer data type. Through the use of C language pointers and the memory allocation functions, it is convenient to do block reads and block writes."
}

@INPROCEEDINGS{dlmalloc,
    author = {A. M. Cheadle and A. J. Field and J. W. Ayres and N. Dunn and R. A. Hayden and J. Nystrom-persson},
    title = {Visualising dynamic memory allocators},
    booktitle = {In Proceedings of the 5th international symposium on Memory management, ISMM ’06},
    year = {2006},
    pages = {115--125}
}

@incollection{BARRY2012227,
title = "Chapter 8 - Embedded Linux",
editor = "Peter Barry and Patrick Crowley",
booktitle = "Modern Embedded Computing",
publisher = "Morgan Kaufmann",
address = "Boston",
pages = "227 - 268",
year = "2012",
isbn = "978-0-12-391490-3",
doi = "https://doi.org/10.1016/B978-0-12-391490-3.00008-4",
url = "http://www.sciencedirect.com/science/article/pii/B9780123914903000084",
author = "Peter Barry and Patrick Crowley"
}

@article{LEE2018359,
title = "Optimal hyperparameter tuning of convolutional neural networks based on the parameter-setting-free harmony search algorithm",
journal = "Optik",
volume = "172",
pages = "359 - 367",
year = "2018",
issn = "0030-4026",
doi = "https://doi.org/10.1016/j.ijleo.2018.07.044",
url = "http://www.sciencedirect.com/science/article/pii/S0030402618310167",
author = "Woo-Young Lee and Seung-Min Park and Kwee-Bo Sim",
keywords = "CNN, Hyperparameter tuning, PSF-HS algorithm",
abstract = "Hyperparameters determine layer architecture in the feature extraction step of a convolutional neural network (CNN), and this affects classification accuracy and learning time. In this paper, we propose a method to improve CNN performance by hyperparameter tuning in the feature extraction step of CNN. In the proposed method, the hyperparameter is adjusted using a parameter-setting-free harmony search (PSF-HS) algorithm, which is a metaheuristic optimization method. In the PSF-HS algorithm, the hyperparameter to be adjusted is set as the harmony, and harmony memory is generated after generating the harmony. Harmony memory is updated based on the loss of a CNN. A simulation using CNN architecture with reference to LeNet-5 and a MNIST dataset, and a simulation using the CNN architecture with reference to CifarNet and a Cifar-10 dataset are performed. By two simulations, it is possible to improve the performance by tuning the hyperparameters in CNN architectures proposed in the past."
}
