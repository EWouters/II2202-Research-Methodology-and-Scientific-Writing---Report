{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantitative exercise: Memory allocator algorithms comparison\n",
    "\n",
    "### by Drago»ô Perju <dsperju@kth.se> and Erik Wouters <ehwo@kth.se>\n",
    "\n",
    "For the purpose of this exercise, we are comparing two dynamic memory allocator algorithms, that have been subjected to a very simple pattern of `malloc` calls. The first algorithm is the **sequential allocator**, that searches the memory bit-by-bit to find free memory to then allocate the requested object. The second algorithm is the buddy memory allocator. A good example of it is offered on Wikipedia [1].\n",
    "\n",
    "In short, however, buddy memory allocator starts with considering the memory to be of size of power of 2 closest to its real size. As an object way smaller in size to be allocated is requested, the buddy memory allocator divides equally the memory in half, then selects the first half and divides it again, and so on until it reaches a power of 2 chunk size slightly bigger than the object size requested. It then assigns the whole chunk to the object, even if the chunk is a tad bigger than the object. By diving the memory into chunks of powers of 2 in size, to be then searched when objects are requested to be allocated, the buddy memory allocator is faster than the sequential memory allocator by O(log n).\n",
    "\n",
    "Our data looks like this. Our data has been fabricated to what we expect to happen in real life conditions:\n",
    "\n",
    "![test](img/screenshot.png)\n",
    "\n",
    "The memory we consider is of size 512 (we can say bytes) and the chunk size is 8 bytes for the buddy memory allocator. The sequential memory allocator can allocate per byte-level, unlike the buddy memory allocator which can now allocate only every 8 bytes along the memory.\n",
    "\n",
    "Below, we're showing the power of the pivot table applied to the data shown above. More so, this whole document has been created using Jupyter Notebook, which can embed code, such as Python code shown below, and execute it in the same interface, as where this text is written, and show the output (being the pivot table and charts) in the same place as well.\n",
    "\n",
    "In this way, the reader can play with the data without compiling anything.\n",
    "\n",
    "[1] https://en.wikipedia.org/wiki/Buddy_memory_allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"500\"\n",
       "            src=\"pivottablejs.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f54145d0400>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "\n",
    "from pivottablejs import pivot_ui\n",
    "\n",
    "pivot_ui(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We however kept the following pictures of the graph-generator above, to point out some conclusions of our data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![test](img/screenshot2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![test](img/screenshot3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above graph we show the fragmentation of both allocator algorithms. The sequential allocator has 0 fragmentation always, because of the simplistic allocation pattern chosen and also because the sequential allocator allocates the objects exactly one next to another, leaving no free space. The buddy memory allocator however, does indeed leave space in between the allocated objects intentionally, having however faster speed because of this as we've seen in the other 2 graphs.\n",
    "\n",
    "For example, if 7 bytes are to be allocated, the buddy memory allocator will choose the bigger power of 2 close to 7, being 8, for allocation. As such, 1 byte remains always free during the lifetime of the object, due to the allocator logic. This is why it is interesting that, if we allocate objects exactly of size of power of 2, the buddy memory allocator leaves no fragmentation. This is what the above graph shows, which is interesting and important, because if we can allocate objects always of size power of 2's, we can have a faster allocator than the sequential one, with no free space loss due to fragmentation - same as the sequential one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![test](img/screenshot4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the above graph, we show how, by allocating 1,2..10 objects each time of sizes 1,2,3..32 bytes, the time spent for allocation under the Buddy Memory Alloctor grows logarithmically instead of linearly, as for the Sequential Allocator. This is to be expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our independent variables were the object sizes to be allocated and the times we allocated - both essentially comprising the allocation pattern. Our dependent variables are the fragmentation and the time taken to execute every allocation. That is something we study and depends on what allocation pattern we're choosing to do. For example, our small study in this exercise does not use deallocation at all - which would increase the fragmentation as holes in the allocated heap would appear and be un-allocatable by bigger objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feedback**: The most important feedback we received is that the second graph above, showing the fragmentation for both allocation algorithms, has its Y axis repeating the number 0.01. Those are actually five different numbers all rounding to 0.01 and it is a default setting of the library used that we did not have time to figure out how to configure it fully to our liking. We think, however, that the tools used and the library used helps with easy reproduction of our small experiment, which is important in research.\n",
    "\n",
    "We would have also continued the exercise, if we had time, to show a Pareto graph. This graph is used when multiple  choices are considered correct - such as choosing the sequential allocator for better fragmentation management, although it's slower, and choosing the buddy memory allocator when speed is needed, although fragmentation is noticable.\n",
    "\n",
    "An example of a Pareto graph is shown below. The X axis could have represented time, while the Y axis could have represented fragmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![text](img/pareto.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thank you for reading!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
