@article{Vinkler2015,
author = {Vinkler, M. and Havran, V.},
title = {Register Efficient Dynamic Memory Allocator for GPUs},
journal = {Computer Graphics Forum},
volume = {34},
number = {8},
pages = {143-154},
year = {2015},
keywords = {dynamic memory allocation, many-core architecture, GPU, CUDA, D.1.3 Programming Techniques: Concurrent Programming Parallel programming D.3.3 Programming Languages: Language Constructs and Features Dynamic storage management D.4.2 Operating Systems: Storage Management Allocation/deallocation strategies},
doi = {10.1111/cgf.12666},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.12666},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.12666},
abstract = {Abstract We compare five existing dynamic memory allocators optimized for GPUs and show their strengths and weaknesses. In the measurements, we use three generic evaluation tests proposed in the past and we add one with a real workload, where dynamic memory allocation is used in building the k-d tree data structure. Following the performance analysis we propose a new dynamic memory allocator and its variants that address the limitations of the existing dynamic memory allocators. The new dynamic memory allocator uses few resources and is targeted towards large and variably sized memory allocations on massively parallel hardware architectures.}
}

@book{bock2001,
  title={Getting It Right: R\&D Methods for Science and Engineering},
  author={Bock, P. and Scheibe, B.},
  isbn={9780121088521},
  lccn={2001089411},
  url={https://books.google.nl/books?id=YS-SaIM3SKAC},
  year={2001},
  publisher={Elsevier Science}
}

@inproceedings{abadi2016,
title	= {TensorFlow: A system for large-scale machine learning},
author	= {Martin Abadi and Paul Barham and Jianmin Chen and Zhifeng Chen and Andy Davis and Jeffrey Dean and Matthieu Devin and Sanjay Ghemawat and Geoffrey Irving and Michael Isard and Manjunath Kudlur and Josh Levenberg and Rajat Monga and Sherry Moore and Derek G. Murray and Benoit Steiner and Paul Tucker and Vijay Vasudevan and Pete Warden and Martin Wicke and Yuan Yu and Xiaoqiang Zheng},
year	= {2016},
URL	= {https://www.usenix.org/system/files/conference/osdi16/osdi16-abadi.pdf},
booktitle	= {12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16)},
pages	= {265--283}
}

===
The below has this good quote:
"These operations will incur significant overheads since serial processing, memory allocation and deallocation greatly limits the performance on the low-frequency processors (i.e.mic cores)."
===
@article{YOU2015,
title = "Scaling Support Vector Machines on modern HPC platforms",
journal = "Journal of Parallel and Distributed Computing",
volume = "76",
pages = "16 - 31",
year = "2015",
note = "Special Issue on Architecture and Algorithms for Irregular Applications",
issn = "0743-7315",
doi = "https://doi.org/10.1016/j.jpdc.2014.09.005",
url = "http://www.sciencedirect.com/science/article/pii/S0743731514001683",
author = "Yang You and Haohuan Fu and Shuaiwen Leon Song and Amanda Randles and Darren Kerbyson and Andres Marquez and Guangwen Yang and Adolfy Hoisie",
keywords = "Machine learning models, Dynamic modeling, Support Vector Machine, Multi- & many-core architectures, Optimization techniques, Performance analysis",
abstract = "Support Vector Machines (SVM) have been widely used in data-mining and Big Data applications as modern commercial databases start to attach an increasing importance to the analytic capabilities. In recent years, SVM was adapted to the field of High Performance Computing for power/performance prediction, auto-tuning, and runtime scheduling. However, even at the risk of losing prediction accuracy due to insufficient runtime information, researchers can only afford to apply offline model training to avoid significant runtime training overhead. Advanced multi- and many-core architectures offer massive parallelism with complex memory hierarchies which can make runtime training possible, but form a barrier to efficient parallel SVM design. To address the challenges above, we designed and implemented MIC-SVM, a highly efficient parallel SVM for x86 based multi-core and many-core architectures, such as the Intel Ivy Bridge CPUs and Intel Xeon Phi co-processor (MIC). We propose various novel analysis methods and optimization techniques to fully utilize the multilevel parallelism provided by these architectures and serve as general optimization methods for other machine learning tools. MIC-SVM achieves 4.4–84× and 18–47× speedups against the popular LIBSVM, on MIC and Ivy Bridge CPUs respectively, for several real-world data-mining datasets. Even compared with GPUSVM, running on the NVIDIA k20x GPU, the performance of our MIC-SVM is competitive. We also conduct a cross-platform performance comparison analysis, focusing on Ivy Bridge CPUs, MIC and GPUs, and provide insights on how to select the most suitable advanced architectures for specific algorithms and input data patterns."
}

=====
We will use it in the main report :D
=====
@incollection{GINSBURG2017345,
title = "Chapter 16 - Application case study—machine learning",
editor = "David B. Kirk and Wen-mei W. Hwu",
booktitle = "Programming Massively Parallel Processors",
publisher = "Morgan Kaufmann",
edition = "Third Edition",
pages = "345 - 367",
year = "2017",
isbn = "978-0-12-811986-0",
doi = "https://doi.org/10.1016/B978-0-12-811986-0.00016-9",
url = "http://www.sciencedirect.com/science/article/pii/B9780128119860000169",
author = "Boris Ginsburg",
keywords = "Convolutional neural network, machine learning, deep learning, matrix–matrix multiplication, forward propagation, gradient backpropagation, training, cuDNN",
abstract = "This chapter provides an application study of how CUDA and GPU computing helped to enable deep learning and revolutionize the field of machine learning. It starts by introducing the basic concepts of convolutional neural networks (CNN). It then shows the CNN code examples that have been accelerated with CUDA. The chapter concludes with an explanation of how the cuDNN library uses a matrix multiplication formulation of the convolution layer computation to improve the speed and utilization of the GPU."
}



??? needed ???
@book{brent_s._baxter_standard_1982,
	address = {New York  {N.Y.}},
	title = {A standard format for digital image exchange},
	isbn = {9780883184080},
	publisher = {Published for the American Association of Physicists in Medicine by the American Institute of Physics},
	author = {{Brent S. Baxter} and {Lewis E. Hitchner} and {Gerald Q. Maguire Jr.}},
	year = {1982}
}

 @inproceedings{Wald_Havran_2006, title={On building fast kd-Trees for Ray Tracing, and on doing that in O(N log N)}, ISBN={978-1-4244-0693-7}, url={doi.ieeecomputersociety.org/10.1109/RT.2006.280216}, DOI={10.1109/RT.2006.280216}, abstractNote={Though a large variety of efficiency structures for ray tracing exist, kd-trees today seem to slowly become the method of choice. In particular, kd-trees built with cost estimation functions such as a surface area heuristic (SAH) seem to be important for reaching high performance. Unfortunately, most algorithms for building such trees have a time complexity of O(N log2 N), or even O(N2). In this paper, we analyze the state of the art in building good kd-trees for ray tracing, and eventually propose an algorithm that builds SAH kd-trees in O(N log N), the theoretical lower bound}, booktitle={IEEE Symposium on Interactive Ray Tracing 2006(RT)}, author={Wald, I. and Havran, V.}, year={2006}, pages={61–69} }
 
 @article{Rademacher_1997, title={Ray Tracing: Graphics for the Masses}, volume={3}, ISSN={1528-4972}, DOI={10.1145/270955.270962}, number={4}, journal={XRDS}, author={Rademacher, Paul}, year={1997}, month={May}, pages={3–7} }

 @inproceedings{Puaut_2002, title={Real-time performance of dynamic memory allocation algorithms}, DOI={10.1109/EMRTS.2002.1019184}, abstractNote={Dynamic memory management is an important aspect of modern software engineering techniques. However developers of real-time systems avoid using it because they fear that the worst-case execution time of the dynamic memory allocation routines is not bounded or is bounded with an excessively large bound. The degree to which this concern is valid is quantified in this paper by giving detailed average and worst-case measurements of the timing performance of a comprehensive panel of dynamic memory allocators. For each allocator we compare its worst-case behavior obtained analytically with the worst timing behavior observed by executing real and synthetic workloads, and with its average timing performance. The results provide a guideline to developers of real-time systems to choose whether to use dynamic memory management or not, and which dynamic allocation algorithm should be preferred from the viewpoint of predictability.}, booktitle={Proceedings 14th Euromicro Conference on Real-Time Systems. Euromicro RTS 2002}, author={Puaut, I.}, year={2002}, month={Jun}, pages={41–49} }

 @article{Ramsundar_Kearnes_Riley_Webster_Konerding_Pande_2015, title={Massively Multitask Networks for Drug Discovery}, url={http://arxiv.org/abs/1502.02072}, abstractNote={Massively multitask neural architectures provide a learning framework for drug discovery that synthesizes information from many distinct biological sources. To train these architectures at scale, we gather large amounts of data from public sources to create a dataset of nearly 40 million measurements across more than 200 biological targets. We investigate several aspects of the multitask framework by performing a series of empirical studies and obtain some interesting results: (1) massively multitask networks obtain predictive accuracies significantly better than single-task methods, (2) the predictive power of multitask networks improves as additional tasks and data are added, (3) the total amount of data and the total number of tasks both contribute significantly to multitask improvement, and (4) multitask networks afford limited transferability to tasks not in the training set. Our results underscore the need for greater data sharing and further algorithmic innovation to accelerate the drug discovery process.}, note={arXiv: 1502.02072}, journal={arXiv:1502.02072 [cs, stat]}, author={Ramsundar, Bharath and Kearnes, Steven and Riley, Patrick and Webster, Dale and Konerding, David and Pande, Vijay}, year={2015}, month={Feb} }

@online{cucumber,
author = {{Google Cloud Blog}},
title = {{"How} a Japanese cucumber farmer is using deep learning and TensorFlow"},
url = {https://cloud.google.com/blog/products/gcp/how-a-japanese-cucumber-farmer-is-using-deep-learning-and-tensorflow/},
note = {Accessed on 2018-09-19}
}

@article{DBLP:HeZRS15,
  author    = {Kaiming He and
               Xiangyu Zhang and
               Shaoqing Ren and
               Jian Sun},
  title     = {Deep Residual Learning for Image Recognition},
  journal   = {CoRR},
  volume    = {abs/1512.03385},
  year      = {2015},
  url       = {http://arxiv.org/abs/1512.03385},
  archivePrefix = {arXiv},
  eprint    = {1512.03385},
  timestamp = {Mon, 13 Aug 2018 16:46:56 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/HeZRS15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@incollection{TRAISTER199099,
title = "Chapter 7 - Pointers and Memory Allocation Functions",
editor = "Robert J. Traister",
booktitle = "Mastering C Pointers",
publisher = "Academic Press",
pages = "99 - 121",
year = "1990",
isbn = "978-0-12-697408-9",
doi = "https://doi.org/10.1016/B978-0-12-697408-9.50011-7",
url = "http://www.sciencedirect.com/science/article/pii/B9780126974089500117",
author = "Robert J. Traister",
abstract = "Publisher Summary
This chapter discusses memory allocation functions in relation to pointers. When a pointer is assigned the memory address of an allocated block of memory, then the pointer has been initialized and is sized to the number of bytes in the storage area. It is then safe to copy objects to this memory block using the pointer for access. The pointer is treated as an array of the same size as the memory block the pointer addresses. Both of the standard C language memory allocation functions will return a NULL character if the amount of memory requested cannot be allocated. Any program that uses either of these functions must test their returns for the NULL. In the Turbo C compiler and most other modern-day equivalents, malloc()and calloc() return pointers of type void. This means that the return address value can assume any legal pointer data type. Through the use of C language pointers and the memory allocation functions, it is convenient to do block reads and block writes."
}